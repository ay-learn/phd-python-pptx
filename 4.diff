- La formule utilisée pour convertir la température de Celsius en Fahrenheit est#cite("conversionDeLatemperature") :
+ La formule utilisée pour convertir la température de Celsius en Fahrenheit est <fc> :

- $ F = 9/5 C + 32 $
+ $ F = 9/5 C + 32 $ <fc>

- Dans cette formule, nous pouvons définir deux facteurs principaux : le poids des entrées et l'ordonnée à l'origine.
+ Dans l’équation @fc, nous pouvons définir deux facteurs principaux : le poids des entrées et l’ordonnée à l’origine.

- Le poids des entrées est un nombre qui multiplie la valeur de la variable indépendante (°C) pour déterminer son effet sur la valeur de la variable dépendante (°F). Dans ce cas, le poids des entrées est 1.8. L'ordonnée à l'origine est un nombre qui est ajouté au produit du poids des entrées par la valeur de la variable indépendante pour déterminer la valeur de la variable dépendante lorsque la variable indépendante est égale à zéro. Dans ce cas, l'ordonnée à l'origine est 32.
+ Le poids des entrées est un nombre qui multiplie la valeur de la variable indépendante (°C) pour déterminer son effet sur la valeur de la variable dépendante (°F). Dans ce cas, le poids des entrées est 1.8. L’ordonnée à l’origine est un nombre qui est ajouté au produit du poids des entrées par la valeur de la variable indépendante pour déterminer la valeur de la variable dépendante lorsque la variable indépendante est égale à zéro. Dans ce cas, l’ordonnée à l’origine est 32.

- E = 1/n sum (y - y_0)^2
+ E = 1/n sum (y - y_0)^2 <emse>

- Où $n$ est le nombre de paires d'entrées et de résultats cibles, $y$ (ou $°F$) est la mesure du résultat généré et $y_0$ (ou $°F_0$) est la mesure du résultat cible.
+ Où $n$ est le nombre de paires d'entrées et de résultats cibles, $y$ (ou $°F$) est la mesure du résultat généré et $y_0$ (ou $°F_0$) est la mesure du résultat cible dans l'équation @emse.

- w_(n+1) = w_n - epsilon (∂E)/(∂w)
+ w_(n+1) = w_n - epsilon (∂E)/(∂w) <wup>

- b_(n+1) = b_n - epsilon (∂E)/(∂b)
+ b_(n+1) = b_n - epsilon (∂E)/(∂b) <bup>

- Où $epsilon$ est appelé taux d'apprentissage et est un petit nombre positif qui contrôle la taille du pas que nous prenons à chaque itération pour réduire la différence entre les résultats attendus et initiaux. $(∂E)/(∂w)$ et $(∂E)/(∂b)$ sont les dérivées partielles de la fonction de coût par rapport aux poids et aux biais respectivement.
+ Où $epsilon$ est appelé taux d'apprentissage et est un petit nombre positif qui contrôle la taille du pas que nous prenons à chaque itération pour réduire la différence entre les résultats attendus et initiaux dans les équations @wup et @bup. $(∂E)/(∂w)$ et $(∂E)/(∂b)$ sont les dérivées partielles de la fonction de coût par rapport aux poids et aux biais respectivement.

- $ f(x) = max(0,x) $
+ $ f(x) = max(0,x) $ <relu>

- L'une des fonctions d'activation les plus simples est l'unité linéaire rectifiée, ou fonction *ReLU*, qui est une fonction linéaire par morceaux qui renvoie zéro si son entrée est négative et renvoie directement l'entrée sinon:
+ L’une des fonctions d’activation les plus simples est l’unité linéaire rectifiée, ou fonction *ReLU*, qui est une fonction linéaire par morceaux qui renvoie zéro si son entrée est négative et renvoie directement l’entrée sinon, comme définie dans l'équation @relu.

- $ E = 1/n sum (y - y_0)^2\ = 4480 $
+ $ E = 1/n sum (y - y_0)^2\ = 4480 $ <emsec>

- Il s'agit d'une explication de l'algorithme de descente de gradient#cite("Goodfellowetal2016") qui est utilisé pour trouver les meilleurs poids d'entrée et les biais afin que la valeur de la fonction de coût soit réduite à zéro.
+ Il s’agit d’une explication de l’algorithme de descente de gradient#cite("Goodfellowetal2016") qui est utilisé pour trouver les meilleurs poids d’entrée et les biais afin que la valeur de la fonction de coût @emsec soit réduite à zéro.

- $ y =

  (
     mat(x_1, x_2)

     mat(
       w_11^((1)),   w_12^((1)) , w_13^((1)), w_14^((1));
       w_21^((1)),   w_22^((1)) , w_23^((1)), w_24^((1));
     )
     +
     mat( b_1^((1)) , b_2^((1)) , b_3^((1)) , b_4^((1)); )
  )
  vec(1, 1, 1, 1)
$
+ $ y =

  (
     mat(x_1, x_2)

     mat(
       w_11^((1)),   w_12^((1)) , w_13^((1)), w_14^((1));
       w_21^((1)),   w_22^((1)) , w_23^((1)), w_24^((1));
     )
     +
     mat( b_1^((1)) , b_2^((1)) , b_3^((1)) , b_4^((1)); )
  )
  vec(1, 1, 1, 1)
$ <yn>

- Cette étude vise à passer en revue les principaux types de réseaux de neurones utilisés dans le domaine de l'imagerie médicale, en particulier les images scanner.
+ Cette étude vise à passer en revue les principaux types de réseaux de neurones utilisés dans le domaine de l'imagerie médicale, en particulier les images scanner, comme décrit dans l'équation @yn.
