# ---------------------------------------------------------
- title: INTRODUCTION GÉNÉRALE
  paragraphs_1:
    - Le cancer du poumon est une cause majeure de décès par cancer.
    - Les scanners thoraciques peuvent repérer des nodules pulmonaires, mais les algorithmes de détection comportent deux étapes; une recherche sensible générant de nombreux faux positifs, suivie d'une réduction de ces faux positifs grâce à des caractéristiques améliorées.
    - Cette étude a utilisé un réseau neuronal convolutionnel sur des scanners thoraciques des patients, montrant une performance de détection et classification de nodules.
  page: page 1/24

# ---------------------------------------------------------

- title: ANATOMIE DES NODULES PULMONAIRES
  subtitle_1: introduction
  paragraphs_1:
    - Les nodules pulmonaires sont des masses anormales qui se forment dans les poumons.
    - Bien qu'ils puissent être bénins, ils peuvent aussi être le signe d'un cancer du poumon.
    - Les nodules pulmonaires sont souvent détectés lors de l'imagerie médicale, comme une radiographie pulmonaire ou un scanner.
  image_path: "../../images/nod2.png"
  page: page 2/24

# ---------------------------------------------------------
#
- title: TYPES DES NODULES PULMONAIRES
  subtitle_1:
  paragraphs_1:
    - Les nodules pulmonaires sont de petites masses arrondies dans les poumons et détectés par des examens d'imagerie médicale (IRM, CT SCAN, …) il existe plusieurs types;
    - néoplasiques
    - inflammatoires
    - Vasculaires
    - Ces nodules peuvent naître de diverses causes comme les infections, le tabac etc..
  page: page 2/24

# ---------------------------------------------------------

- title: Anatomie du poumon
  paragraphs_1:
    - L'anatomie pulmonaire concerne la structure des poumons, essentielle à l'échange de gaz vitaux.
    - Ils se trouvent dans le thorax, séparés par le médiastin.
  image_path: "../../images/poum_str.png"
  page: page 3/24

- title: Morphologie interne et externe et vascularisation
  # paragraphs_1:
  #   -
  page: page 4/24

# ---------------------------------------------------------

- title: Causes, stades du cancer de poumon.
  paragraphs_1:
    -  Le cancer du poumon, une maladie qui voit des cellules anormales croître de manière incontrôlée dans le tissu pulmonaire, découle principalement de l’exposition à la cigarette, le tabagisme passif, l’exposition professionnelle à des substances cancérigènes, il existe 4 stades du cancer du poumon ;
    - Stade I ; tumeur limitée au poumon, sans atteinte des ganglions lymphatiques ni métastases à distance.
    - Stade II ; la tumeur a envahi les ganglions lymphatiques proches du poumon touché ou a infiltré les structures avoisinantes.
    - Stade III ; la tumeur est plus avancée localement et a envahi plus de ganglions lymphatiques ou de structures adjacentes.
    - Stade IV ; le cancer a migré vers d’autres parties du corps, souvent éloignées des poumons.
  image_path: "../../images/stad_du_cancer.png"
  page: page 5/24

# ---------------------------------------------------------

- title: Conclusion
  paragraphs_1:
    - Ce chapitre explore en détail l'anatomie des nodules pulmonaires, leur importance dans la détection précoce du cancer, et les options de traitement.
    - L'utilisation de l'Intelligence Artificielle de type Deep-Learning avec l'ensemble de données LIDC-IDRI permet de détecter et classifier les nodules pulmonaires avec précision, améliorant ainsi les chances de survie des patients.
  page: page 6/24

# ---------------------------------------------------------

- title: INTRODUCTION
  paragraphs_1:
    - Le deep learning ou l'apprentissage profond est une  méthode
      d'apprentissage automatique qui permet à une machine d'apprendre à partir
      des données en utilisant des réseaux de neurones artificiels.
  image_path: "../../images/ais.png"
  page: page 7/24

- title: FONCTIONNEMENT DU DEEP LEARNING
  paragraphs_1:
    - Le fonctionnement du Deep-Learning est basé sur des réseaux de neurones
      imitant le cerveau humain.
  image_path: "../../images/fonctionement_y.png"
  page: page 8/24

- title: APPLICATIONS DE DEEP LEARNING
  paragraphs_1:
    - En plein essor depuis une dizaine d'années, le Deep-Learning est utile
      dans divers domaines.
  page: page 9/24

# ---------------------------------------------------------

- title: RÉSEAUX DE NEURONES ET DEEP LEARNING
  paragraphs_1:
    - Un réseau de neurones est une structure consistant en un ensemble
      (couche) de briques élémentaires (neurones) effectuant chacune des
      opérations simples.
    - Couche d'entrée
    - Couche de traitement
    - Couche de sortie
  image_path: "../../images/reseuxActicvation.png"
  page: page 10/24

# ---------------------------------------------------------

- title: IMAGE ET DEEP LEARNING
  paragraphs_1:
    - L'apprentissage profond est très efficace pour l'analyse des images,
      que soit pour la classification ou la segmentation, en suivant 7 étapes;
    - Collecte des données
    - Prétraitement des données
    - Création d'un modèle de deep learning
    - Entraînement du modèle
    - Validation et ajustement
    - Test du modèle
    - Déploiement
  image_path: "../../images/structure_resnet.png"
  page: page 12/24

# ---------------------------------------------------------

- title: CONCLUSION
  paragraphs_1:
    - conclusions
  page: page 13/20

# ---------------------------------------------------------

- title: LE PROCESSUS DE CRÉATION D'UN MODÈLE IA
  subtitle_1: La phase de prétraitement des données
  paragraphs_1:
    # - comment L'utilisation de l'IA pour la détection et la classification
    #   des nodules pulmonaires comporte deux parties essentielles.
    - Consiste à détecter un grand nombre de nodules avec une grande sensibilité,
      mais cela génère des faux positifs. Ensuite, la phase de détection réduit ces faux positifs.
    - Normalisation des nodules pour avoir la même taille, les mêmes canaux de couleur, etc.
  image_path: "../../images/model2_a.png"
  page: page 14/24

# ---------------------------------------------------------

- title: LE PROCESSUS DE CRÉATION D'UN MODÈLE IA
  subtitle_1: La phase d'entraînement du modèle
  paragraphs_1:
    - L'entraînement du modèle sur des images de nodules ou de lésions, par exemple,
      modifie les paramètres du modèle pour devenir capable de créer certains motifs
      qui révèlent les caractéristiques et les informations contenues dans les images.
    - Ensuite, on peut classer ces motifs en certaines classes. Dans notre cas, il s'agit
      de la classification binaire, avec deux classes; nodule ou non-nodule.
  image_path: "../../images/model2_b.png"
  page: page 15/24

# ---------------------------------------------------------

- title: LE PROCESSUS DE CRÉATION D'UN MODÈLE IA
  subtitle_1: La phase de prédiction
  paragraphs_1:
    - Lorsqu'une nouvelle image, quelle que soit sa conception ou sa forme, est
      introduite dans le modèle, elle est convertie grâce aux couches de CNN pour
      devenir plus similaire à l'un des motifs sur lesquels le modèle a été entraîné
      pour la comprendre.

    # - On commence par créer un modèle qui contient des paramètres aléatoires,
    # et cela génère des motifs aléatoires. Cependant, grâce à l'entraînement
    # sur des exemples réels bien traités, le modèle devient capable d'extraire
    # de petites images contenant des motifs caractérisant les informations
    # complexes des nodules. Ainsi, il réduit la complexité des images de
    # grande dimension en des images plus petites, pouvant être classifiées
    # dans certaines classes.
  image_path: "../../images/model2_c.png"
  page: page 16/24

# ---------------------------------------------------------

- title: MATÉRIEL ET MÉTHODE
  subtitle_1: "Les en jeu de donnée"
  paragraphs_1:
    # - Bonjour. Merci mon camarade pour ces informations importantes
    #   Maintenant, passons à la partie pratique.
    # - Dans notre étude, nous avons élaboré deux modèles de Deep Learning,
    #   l'un pour détecter les nodules et l'autre pour la classification.
    # - Nous utilisons comme source des images radiologiques annotées par
    #   quatre experts. Ces images proviennent des ensembles de données
    #   LIDC-IDRI et LUNA16, qui sont disponibles en ligne en open source.
    #   Nous avons également créé un ensemble de données appelé TRPMLN.

    - Les ressources utilisées dans notre étude comprenaient des scans CT et
      des annotations provenant du dataset LIDC-IDRI, du dataset LUNA16 et
      TRPMLN.
    - Le dataset LIDC-IDRI est une base de données publique contenant 1018
      scans thoraciques annotés par quatre radiologues experts. Chaque nodule
      pulmonaire est décrit dans un fichier XML qui contient son
      identifiant, ses caractéristiques, et sa région d'intérêt.
    - Nous avons utilisé des ordinateurs équipés de GPUs stockés sur un site
      Web de recherche appelé Kaggle pour effectuer nos analyses.
  image_path: "../../images/martiel.png"
  page: page 17/24

# ---------------------------------------------------------

- title: MATÉRIEL ET MÉTHODE.
  subtitle_1: Les jeux de données LUNA16/TRPMLN.
  paragraphs_1:
    - Pour entraîner notres modèles de détection et de classification en
      utilisant deux ensembles de données adaptés et faciliter leur
      utilisation dans le Deep Learning, nous avons utilisé un sous-ensemble
      de données appelé LUNA16 pour la détection des nodules, et nous avons
      créé un sous-ensemble appelé TRPMLN pour classer les nodules de haut
      risque de malignité.
    - LUNA16 contient 6691 nodules annotés dans 888 scans thoraciques.
    - TRPMLN contient 1568 nodules annotés dans 304 scans thoraciques, en
      extrayant les nodules qui ont une moyenne de malignité égale à 3 ou plus
      dans les annotations des quatre experts.
  image_path: "../../images/type_nodules.png"
  page: page 18/24

# ---------------------------------------------------------

- title: ARCHITECTURE
  subtitle_1: "CNN & ResNET"
  paragraphs_1:
    # - Comme architecture du modèle, on utilise des couches CNN dans le
    #   framework TensorFlow.
    - Nous avons expérimenté différentes combinaisons de couches et de filtres
      pour trouver la meilleure hyerparameter de CNNs architecture pour notre problème.
    - Pour entraîner le modèle, on utiliser l'optimiseur Adam avec un taux
      d'apprentissage de 0,001, et la fonction de perte s'appelle l'entropie
      croisée binaire.
  image_path: "../../images/architicture.png"
  page: page 19/24

# ---------------------------------------------------------

- title: RÉSULTATS DE MODEL DE DÉTECTION
  subtitle_1: "Résultat d'entraînement"
  paragraphs_1:
    # - Nous avons divisé le DATASET en deux parties pour cacher une partie des
    #   données en vue de l'évaluation du modèle, car la validation du modèle sur
    #   de nouveaux candidats nous donne la vraie performance du modèle.
    #
    # - Ces courbes représentent l'état d'entraînement du modèle.
    # - La courbe bleue correspond à la précision obtenue lors de l'entraînement
    #   avec les nodules d'entraînement.
    # - La courbe jaune correspond à la précision obtenue lors de l'évaluation
    #   avec les nodules de test (comme de nouveaux nodules).
    #
    # - L'essentiel est que le graphe de validation atteigne la meilleure valeur.
    #
    # - Il est très important de visualiser l'état d'entraînement, époque par
    #   époque, pour voir si le modèle s'entraîne correctement ou non.
    #
    # - Si la précision est égale à 50%, on parle de prédiction aléatoire, donc
    #   le modèle ne comprend rien.
    # - Si le modèle s'entraîne un peu et que la précision n'augmente pas, on
    #   arrête l'entraînement et on ajuste d'autres hyperparamètres.
    # - Si la courbe d'entraînement augmente et que la courbe de validation
    #   n'augmente pas, le modèle mémorise les nodules au lieu de comprendre le
    #   motif.

    - En examinant les valeurs de l’exactitude et de l’exactitude de validation
      tout au long des étapes d'apprentissage, on constate que le modèle
      acquiert des connaissances, comme le montre l'amélioration progressive
      des précisions d'entraînement et de validation.
    - Le modèle commence avec des précisions relativement plus faibles, autour
      de 64%, avant d'augmenter à plus de 89% et de terminer avec un score de
      87% à la fin de l'entraînement.
    - Cela démontre la capacité affinée du modèle à classer correctement un
      pourcentage considérable de cas.

    # - À l'aide de la fonction de coût, qui calcule l'erreur entre les valeurs
    #   approximées et les valeurs réelles, nous utilisons l'optimisation Adam
    #   pour améliorer l'approximation de ce modèle de Deep-Learning. Cependant,
    #   plusieurs contraintes empêchent l'optimisation des paramètres du modèle.
    #   Pour résoudre cela, nous ajustons les hyperparamètres afin de trouver des
    #   combinaisons permettant de minimiser les erreurs et d'augmenter la
    #   précision.
    # - Nous testons plusieurs modèles un par un, en les entraînant sur un
    #   certain nombre d'époques et en visualisant les résultats. Nous observons
    #   si les performances des modèles s'améliorent au fil du temps en examinant
    #   les courbes d'entraînement et de validation.
    #
    # - À chaque époque, on calcule l'erreur, on essaie d'optimiser cette erreur,
    #   et on teste la performance en utilisant tous les nodules d'entraînement
    #   pour mettre à jour la courbe d'entraînement. Ensuite, on teste le modèle
    #   avec les nodules de test pour mettre à jour la courbe de validation.
    # - Le meilleur modèle que l'on trouve est bien entraîné sur les candidats
    #   qu'il a déjà vus et affiche une performance de 89% sur les candidats
    #   qu'il n'a jamais vus.
  image_path: "../../images/class2.jpg"
  image_position: IMG_RIGHT
  page: page 20/24

# ---------------------------------------------------------

- title: RÉSULTATS DE MODÈLE DE DÉTECTION
  subtitle_1: "Matrice de confusion"
  paragraphs_1:
    - L'utilisation des 1339 nodules de test que le modèle n'a pas encore vus
      pour la détection des nodules/lésions se traduit par une matrice de
      confusion.
    # - Pour visualiser la signification de ces nombres, on peut utiliser le
    # diagramme de Sankey.
  image_path : "../../images/ch3_tab1.png"
  page: page 21/24

# ---------------------------------------------------------

- title: RÉSULTATS DE MODÈLE DE DÉTECTION
  subtitle_1: "Diagrame de Sankey pour detection"
  # paragraphs_1:
    # - Si l'on passe 882 lésions et 517 nodules à travers le modèle de
    #   détection, on obtient 771 prédictions de vraies lésions et 404
    #   prédictions de vrais nodules.
    # - Malheureusement, le modèle prédit 133 nodules comme étant des lésions et
    #   51 lésions comme étant des nodules.
    # - Comment les erreurs de ces résultats sont exprimées mathématiquement pour
    #   les évaluer et les comparer avec d'autres modèles qui utilisent des
    #   nombres différents de nodules de test.
  image_path : "../../images/sankey_diagram_1.png"
  page: page 22/24

# ---------------------------------------------------------

- title: RÉSULTATS DE MODÈLE DE DÉTECTION
  subtitle_1: Métriques d'évaluation
  paragraphs_1:
    - La performance du modèle peut a été évaluée à partir de la matrice de
      confusion, qui permet de calculer des métriques comme la précision, la
      sensibilité (recall) et le F1-Score, en plus de l’exactitude.
    - Ces mesures fournissent un aperçu plus large des performances du modèle,
      notamment quand il y a un déséquilibre des classes.
    - précision   = (VP) / (VP + FP)
      sensibilité = (VP) / (VP + FN)
      F_1-Score   = (2 VP)/(2VP + FP + FN)
  image_path : "../../images/ch3_tab2.png"
  image_position: IMG_RIGHT
  page: page 23/24

# ---------------------------------------------------------

- title: DISCUSSION DE MODÈLE DE DÉTECTION
  subtitle_1: Overfitting
  paragraphs_1:
    - L'exactitude de l'entraînement atteint 100 %,ce qui est un signe clair
      de surapprentissage(overfitting), surtout par rapport à l'exactitude de
      validation qui est bien inférieure. Le surajustement signifie que le
      modèle a trop bien appris les données d'entraînement.
    - Pour résoudre les problèmes de Overfitting, il faut augmenter le nombre
      d'échantillons ou utiliser des techniques de Deep Learning, telles que
      des techniques de régularisation comme la technique d'intention.
    - Comparaison avec nos résultats, notre modèle de classification de nodule
      ou lésion est performant de manière compétente dans l'identification des
      deux classes.
    - En général, le modèle a performé de manière impressionnante en termes
      de précision, de sensibilité et de "F1-score".
  image_path : "../../images/ch3_tab6.png"
  image_position: IMG_RIGHT
  page: page 24/24

# ---------------------------------------------------------

- title: MODÈLE DE CLASSIFICATION DES NODULE DE HAUT RISQUE DE MALIGNETE
  # subtitle_1: Overfitting
  # paragraphs_1:
  #   - L'exactitude de l'entraînement atteint 100 %,ce qui est un signe clair
  #     de surapprentissage(overfitting), surtout par rapport à l'exactitude de
  #     validation qui est bien inférieure. Le surajustement signifie que le
  #     modèle a trop bien appris les données d'entraînement.
  #   - Pour résoudre les problèmes de Overfitting, il faut augmenter le nombre
  #     d'échantillons ou utiliser des techniques de Deep Learning, telles que
  #     des techniques de régularisation comme la technique d'intention.
  #   - Comparaison avec nos résultats, notre modèle de classification de nodule
  #     ou lésion est performant de manière compétente dans l'identification des
  #     deux classes.
  #   - En général, le modèle a performé de manière impressionnante en termes
  #     de précision, de sensibilité et de "F1-score".
  image_path : "../../images/ch3_tab7.png"
  # image_position: IMG_RIGHT
  page: page 24/24

# ---------------------------------------------------------
